{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb26f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary libraries to build a transformer model \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d592a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are defining a class called MultiHeadAttention which is a subclass of the \n",
    "#attention nn.Module \n",
    "\n",
    "#subclass of nn module \n",
    "class MultiHeadAttention(nn.Module): \n",
    "    \n",
    "    #d_model represents the dimension of the input and output, it is the \n",
    "    #hidden size of the transformer model \n",
    "    #num_heads represent the number of parallel attention heads that is used\n",
    "    def __init__ (self, d_model, num_heads): \n",
    "        \n",
    "        #calls the constructor of the parent class to initialize the object \n",
    "        super(MultiHeadAttention, self). __init__ () \n",
    "        \n",
    "        #makes an assertion that the dimension of the input must be the same as the \n",
    "        #dimension of the output or it will return an error \n",
    "        assert d_model % num_heads == 0 #d_model must be divisible by num_heads \n",
    "        \n",
    "#      this code initializes a MultiHeadAttention module by specifying the input and output dimension \n",
    "#      and the number of attention heads. It then performs a sanity check to ensure that \n",
    "#      the output is divisible by the num heads. \n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "#       the method scaled dot product attention is a class within MultiHeadAttention \n",
    "#       where the attention scores are calculated\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \n",
    "#        mask tensor is used to selectively mask certain positions in the attention score \n",
    "#        it controls which position should be attended to(value set to 1) and which positions \n",
    "#        should be ignored(value set to 0) if mask tensor is provided then and is not 'none' \n",
    "#        then the code masks out certain positions in the attention score by setting the value to a large \n",
    "#        negative number. This is done to prevent attending to certain positions \n",
    "#        such as padding tokens in the input sequence. \n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "#           The line below applies the softmax function along the last \n",
    "#           dimension of the attention scores tensor to obtain the \n",
    "#           attention probabilities. Softmax normalizes the scores, \n",
    "#           ensuring that they sum up to 1. The attention probabilities \n",
    "#           indicate the weights assigned to different positions in the value tensor\n",
    "\n",
    "\n",
    "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "    \n",
    "#           This line computes the weighted sum of the value tensor V using \n",
    "#           the attention probabilities.    \n",
    "    \n",
    "        output = torch.matmul(attn_probs, V) \n",
    "        return output\n",
    "        \n",
    "#         this subclass reshapes the input tensor 'x' into a tensor with \n",
    "#         dimensions (batch_size, seq_length, self.num_heads, self.d_k) \n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "#     this subclass performs the inverse of the split head class. It takes the tensor 'x'\n",
    "#     containing the output of the attention heads and transposes it back to the \n",
    "#     original dimensionality \n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "#         defining the forward subclass of the model, where the linear transformation represented by \n",
    "#         self.W_q, self.W_k, self.W_v is applied to the query tensor 'Q','K', 'V' respectively. The \n",
    "#         linear transformation projects the query tensor to the query space \n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "#         the feed forward network concatenates the attention scores and return the masked output(tokenized->un-tokenize)\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78681112",
   "metadata": {},
   "source": [
    "self.d_model(instance variable) = d_model(constructor argument) assigns the value. This instance variable represents the dimensionality of the input and output of the multi-head attention mechanism.\n",
    "\n",
    "self.num_heads(instance variable) = num_heads(constructor argument)   This instance variable represents the number of parallel attention heads used in the multi-head attention mechanism.\n",
    "\n",
    "self.d_k = d_model // num_heads calculates the dimensionality 'd_k' for each attention head. It is derived by dividing the d_model by the num_heads using the floor division operator (//). This calculation ensures that each attention head has an equal dimensionality, as the total d_model is distributed evenly among the num_heads.\n",
    "\n",
    "The d_k value is important because, in the multi-head attention mechanism, the input tensor is linearly projected into different attention heads. The projection is performed by multiplying the input tensor with projection weight matrices specific to each attention head. The d_k value represents the size of the projected space for each attention head.\n",
    "\n",
    "self.W_q = nn.Linear(d_model, d_model): This line creates an instance of the nn.Linear class and assigns it to the self.W_q instance variable. \n",
    "nn.Linear represents a linear transformation that maps an input tensor of size d_model to an output tensor of size d_model. \n",
    "self.W_q will be responsible for projecting the input tensor to the query space.\n",
    "\n",
    "self.W_k = nn.Linear(d_model, d_model): self.W_k will be responsible for projecting the input tensor to the key space.\n",
    "\n",
    "self.W_v = nn.Linear(d_model, d_model): self.W_v will be responsible for projecting the input tensor to the value space.\n",
    "\n",
    "**Query, Key and Value, refer to the All you Need is Attention paper**\n",
    "\n",
    "self.W_o = nn.Linear(d_model, d_model): self.W_o will be responsible for transforming the concatenated outputs of the attention heads back to the original d_model dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c510ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
