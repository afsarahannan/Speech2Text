{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c1b47e",
   "metadata": {},
   "source": [
    "load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265c5935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-731b845314f578c7\n",
      "Reusing dataset csv (C:\\Users\\Mir Info\\.cache\\huggingface\\datasets\\csv\\default-731b845314f578c7\\0.0.0\\2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Parameter 'generator'=Generator(PCG64) of the transform datasets.arrow_dataset.Dataset.train_test_split couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_file = 'test_t5.csv' #the dataset will be stored in the format (n rows :2 columns{source,target})\n",
    "\n",
    "dataset = load_dataset('csv', data_files=dataset_file, split='train') #this line of command does not do any actual splitting, we want HF to think our custom dataset is actually a built-in dataset on their server   \n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1) #90% training data, this is where it is actually split \n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ad0cb",
   "metadata": {},
   "source": [
    "tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e2fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "\n",
    "df = pd.read_csv('test_t5.csv')\n",
    "\n",
    "source_text = df['source']\n",
    "target_text = df['target']\n",
    "\n",
    "tokenized_source_text = tokenizer(list(source_text), truncation=False, padding=False)\n",
    "tokenized_target_text = tokenizer(list(target_text), truncation=False, padding=False)\n",
    "\n",
    "max_source = 0\n",
    "for item in tokenized_source_text['input_ids']:\n",
    "    if len(item) > max_source:\n",
    "        max_source = len(item)\n",
    "\n",
    "max_target = 0\n",
    "for item in tokenized_target_text['input_ids']:\n",
    "    if len(item) > max_target:\n",
    "        max_target = len(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ff5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    tokenized_input = tokenizer(batch['source'], padding='max_length', truncation=True, max_length=max_source)\n",
    "    tokenized_label = tokenizer(batch['target'], padding='max_length', truncation=True, max_length=max_target)\n",
    "\n",
    "    tokenized_input['labels'] = tokenized_label['input_ids']\n",
    "\n",
    "    return tokenized_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8747a523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37bd4c2d6cd4dec8bc917d01765bc43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6aff20dad041ecbaaf4cc341145ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=512)\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
    "\n",
    "train_dataset.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb19929d",
   "metadata": {},
   "source": [
    "save the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecdcfea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.save_to_disk('t5_train')\n",
    "val_dataset.save_to_disk('t5_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a288fa",
   "metadata": {},
   "source": [
    "training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a229656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a1e66da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getcwd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6060\\1272529679.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'getcwd' is not defined"
     ]
    }
   ],
   "source": [
    "getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3139086d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39707152fa5446e98db52b9ddea3418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=891691430.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "output_dir = 'output/dir'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_accumulation_steps=1, # Number of eval steps to keep in GPU (the higher, the mor vRAM used)\n",
    "    prediction_loss_only=True, # If I need co compute only loss and not other metrics, setting this to true will use less RAM\n",
    "    learning_rate=0.001,\n",
    "    evaluation_strategy='steps', # Run evaluation every eval_steps\n",
    "    save_steps=1000, # How often to save a checkpoint\n",
    "    save_total_limit=1, # Number of maximum checkpoints to save\n",
    "    remove_unused_columns=True, # Removes useless columns from the dataset\n",
    "    run_name='run_name', # Wandb run name\n",
    "    logging_steps=1000, # How often to log loss to wandb\n",
    "    eval_steps=1000, # How often to run evaluation on the val_set\n",
    "    logging_first_step=False, # Whether to log also the very first training step to wandb\n",
    "    load_best_model_at_end=True, # Whether to load the best model found at each evaluation.\n",
    "    metric_for_best_model=\"loss\", # Use loss to evaluate best model.\n",
    "    greater_is_better=False # Best model is the one with the lowest loss, not highest.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir + '/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c14ac",
   "metadata": {},
   "source": [
    "evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'your/model/dir'\n",
    "output_dir = 'your/output/dir'\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "\n",
    "pred_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_eval_batch_size=8,\n",
    "    remove_unused_columns=True,\n",
    "    eval_accumulation_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=pred_args)\n",
    "\n",
    "preds, labels, *_ = trainer.predict(val_dataset)\n",
    "preds_tokens = preds.argmax(axis=2)\n",
    "\n",
    "decoded_sources = []\n",
    "for row in val_dataset:\n",
    "    decoded_sources.append(tokenizer.decode(row['input_ids']))\n",
    "\n",
    "decoded_preds = [tokenizer.decode(pred) for pred in preds_tokens]\n",
    "decoded_labels = [tokenizer.decode(label) for label in labels]\n",
    "\n",
    "output = pd.DataFrame({'Source Text': decoded_sources, 'Target Text': decoded_labels, 'Generated Text': decoded_preds})\n",
    "output.to_excel(output_dir + \"/predictions.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e914b9b",
   "metadata": {},
   "source": [
    "model testing with new input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d952e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"input:\")\n",
    "input_text = input()\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokenized_text = tokenizer(input_text, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "    source_ids = tokenized_text['input_ids'].to(device, dtype = torch.long)\n",
    "    source_mask = tokenized_text['attention_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        input_ids = source_ids,\n",
    "        attention_mask = source_mask, \n",
    "        max_length=512,\n",
    "        num_beams=5,\n",
    "        repetition_penalty=1, \n",
    "        length_penalty=1, \n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "    pred = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "print(\"\\noutput:\\n\" + pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
